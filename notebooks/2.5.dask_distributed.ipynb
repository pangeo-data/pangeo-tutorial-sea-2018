{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5. Dask Distributed\n",
    "\n",
    "Dask `distributed` is a separate package that addresses the limitations of Dask's built-in schedulers:\n",
    "\n",
    "1. Both multithreading and multiprocessing schedulers are *local* schedulers (i.e., on your laptop), so they are limited to memory of a single machine.\n",
    "2. The multithreading scheduler is difficult to scale too far beyond the number of cores (or 2x the number of cores with hyperthreading)\n",
    "3. The multithreading scheduler can be limited by the GIL (though not with Numpy).\n",
    "4. The multiprocessing only allows communications between the scheduler and the workers, not between workers (i.e., hub-and-spoke communication topography).\n",
    "\n",
    "Dask `distributed` provides a solution to all of these problems:\n",
    "\n",
    "1. uses TCP (`tornado`) to communicate between processes, so can be truly distributed (and not local),\n",
    "2. is limited only by the size of the available \"cluster\" (but can actually be multi-cluster!),\n",
    "3. is multi-process-based, so it doesn't contend with the GIL, and\n",
    "4. provides worker-to-worker communication, allowing for efficient and complex task graphs.\n",
    "\n",
    "[Go here](https://distributed.readthedocs.io/en/latest/) for more information about `distributed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Set Up a (Local) Dask Distributed Cluster\n",
    "\n",
    "The easiest way to start a cluster is directly from your notebook using the `Client` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# On your laptop or GCE, run the following\n",
    "from dask.distributed import LocalCluster\n",
    "cluster = LocalCluster(n_workers=4)\n",
    "\n",
    "# On Cheyenne, run the following\n",
    "#from dask_jobqueue import PBSCluster\n",
    "#cluster = PBSCluster(queue='premium', project='STDD0006', processes=1, threads=1, resource_spec='select=1:ncpus=36')\n",
    "#cluster.start_workers(4)\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LocalCluster\n",
    "\n",
    "If you are running on your laptop, you are using just a \"local cluster.\"  But you can see that launching a cluster on any other machine (such as Cheyenne) is similar.  You create the appropriate `*Cluster` object and pass this to the `Client`.  The `dask_jobqueue` package (which must be installed separately with:\n",
    "\n",
    "    pip install git+https://github.com/dask/dask-jobqueue.git\n",
    "\n",
    "in your `pangeo` conda environment) provides additional `*Cluster` objects for `PBSCluster` and `SLURMCluster`, for managing clusters using the PBS and SLURM job schedulers, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard\n",
    "\n",
    "Many of the diagnostics that we've already seen in the last section are available from the **Dashboard** link in the information box returned when we displayed `client` in the above output.  If you click that link, it will take you to a page that provides many of those diagnostics in one place!\n",
    "\n",
    "Each kind of task on the *Status* page of the Dashboard is displayed in block, with start and stop times roecorded for each of the following:\n",
    "\n",
    "1. Serialization (gray)\n",
    "2. Dependency gathering from peers (red)\n",
    "3. Disk I/O to collect local data (orange)\n",
    "4. Execution times (additional colors chosen for each task)\n",
    "\n",
    "displayed in the *Task Stream* section of the Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are many ways to set up a cluster...\n",
    "\n",
    "For more ways to set up a cluster with Dask, [click here](https://distributed.readthedocs.io/en/latest/setup.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Client\n",
    "\n",
    "The `Client` object provides an interface with the main scheduler for your cluster.  It provides a number of functions that you can use directly to run code on the cluster (instead of just the scheduler).  Some of these functions and attributes are described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "\n",
    "Satisfying the `concurrent.futures` standard, you can `map` a function across an iterative object.  The result of `client.map` is a `Future` object that is stored on the worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbl(x):\n",
    "    time.sleep(1)\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply this new `dbl` function to an iterable object (`range`) on the distributed workers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubles = client.map(dbl, range(80))\n",
    "doubles[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUICKLY, go check out the Dashboard while this is running...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubles[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doubles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a list of `Future` objects in our notebook.  \n",
    "\n",
    "*Keep in mind* that our notebook is attached to the *scheduler*.  These `Future` objects point to objects that are stored (in the memory) of the *workers*!  So, we have automatically distributed the data produced by `range(80)` onto the workers by applying `map`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit\n",
    "\n",
    "Now that the data is on the workers, we can apply functions to the entire distributed dataset by using the `submit` method of the `Client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_doubles = client.submit(sum, doubles)\n",
    "display(sum_doubles)\n",
    "time.sleep(8)\n",
    "display(sum_doubles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather\n",
    "\n",
    "With the `doubles` data stored on the workers, we can bring that data to the notebook/scheduler with the `gather` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.gather(doubles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "\n",
    "Do not gather unless you absolutely must!  It is usually much more efficient to keep the data distributed across the cluster and `submit` functions that act on the distributed data than it is to bring the data to the scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter\n",
    "\n",
    "You can do the reverse of `gather` if you want to distribute data that you already have on your scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_data = client.scatter(range(4000))\n",
    "dist_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_data[4].result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute\n",
    "\n",
    "Like the previous discussions, you can use the `Client` `compute` method to force the computation of a task graph created by using Dask Delayed, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def dbl(x):\n",
    "    time.sleep(1)\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def inc(x):\n",
    "    time.sleep(1)\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def dsum(x):\n",
    "    time.sleep(1)\n",
    "    return sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [2, 5, 7, 3, 1, 8, 6, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time sum_odd_data = dsum( inc(dbl(x)) for x in data )\n",
    "sum_odd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = client.compute(sum_odd_data)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "\n",
    "The return value of `Client.compute` is a `Future` object!  That means that, unlike in the previous sections, the `compute` (and `persist`) operations will be done asynchronously!\n",
    "\n",
    "And once the result has been computed, then you can get the result of the `Future` with the `result()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist\n",
    "\n",
    "Similarly, you can `persist` the results of a tash graph (just like in the previous sections) by using the `Client` `persist` method.\n",
    "\n",
    "Because `Client.persist` is asynchronous, we can set up initial data that is distributed *in memory* across the cluster using the `persist` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel & Restart & Close\n",
    "\n",
    "You can cancel a computation on the cluster by using the `Client.cancel` method and passing it the `Future` associated with the computation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can completely kill all `Future`s and restart the cluster with `restart`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can close down a cluster with the `close` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Delayed and Collections (next lesson)\n",
    "\n",
    "Once you have initialized a `Client` (i.e., connected to the scheduler), normal Dask features like Dask Delayed, Dask Array, etc. will work using the client's scheduler by default.  So, no special mechanism is needed to get all of the previous examples to work with `distributed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advice for Efficiency\n",
    "\n",
    "When running with the `distributed` scheduler, there are a few things you should consider for the sake of efficiecy:\n",
    "\n",
    "1. As mentioned above, avoid `gather`ing the data to the scheduler.  Keep the data distributed!\n",
    "2. Avoid creating too many tasks that need to be distributed across the cluster!  (The `distributed` scheduler adds about *1 ms overhead for each task*, depending upon the network.  Thus, as a rule of thumb, don't worry about distributing your tasks if the serial operation takes less than about 100 ms to run.)\n",
    "3. With the `distributed` scheduler, you can configure workers to use multiple threads *per worker*.  (For example, you can create one worker for each remote node on a cluster, and each worker can run as many threads as cores on that node.)\n",
    "4. Tasks are assigned to workers using *heuristics*, so keep in mind that Dask might not get it perfect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
